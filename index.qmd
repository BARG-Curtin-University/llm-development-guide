---
title: "Developing Large Language Models from Scratch: A Comprehensive Guide"
authors:
author:
  - name: Michael Borck
    affiliation: Business Information Systems, Curtin University, Perth Australia
    orcid: 0000-0002-0950-6396
    corresponding: true
    email: michael.borck@curtin.edu.au
    roles:
      - Investigation
      - Project administration
      - Software
      - Visualisation
keywords:
  - "Large Language Models (LLMs)"
  - "Data Curation"
  - "Model Architecture"
  - "Training Techniques"
  - "Evaluation Methods"
abstract: |
  The development of large language models (LLMs) from scratch is a complex and resource-intensive endeavour, often perceived as feasible only for specialised AI research institutions. However, with the rising interest in custom LLMs for specific business and organisational applications, understanding the comprehensive process of building these models has become increasingly relevant. This paper provides a detailed guide on the critical steps involved in developing a foundational LLM, drawing insights from models such as GPT-3, LLaMA, and Falcon. Key aspects covered include data curation, where the quality and diversity of the training data are emphasised; model architecture, focusing on the transformative role of transformer networks and their various configurations; training at scale, which addresses the technical challenges and solutions for efficient large-scale model training; and evaluation, detailing methodologies for assessing model performance across different tasks. This guide aims to equip researchers and practitioners with the knowledge necessary to navigate the intricacies of LLM development and make informed decisions about when such an investment is justified.
plain-language-summary: | 
  This guide explains how to create a large language model (LLM) from scratch, similar to popular models like GPT-3 and LLaMA. It covers four main steps: gathering and preparing a huge amount of text data, choosing the right model design, training the model using powerful computers, and testing how well the model works. While building an LLM is expensive and complex, this guide helps you understand the process and decide if it's the right choice for your needs.
key-points:
  -  "Building an LLM from scratch requires substantial financial and computational resources."
  -  "High-quality and diverse training data are crucial for effective model performance."
  -  "Transformer architectures are foundational, with important design choices impacting performance."
  -  "Advanced techniques like mixed precision training and 3D parallelism are key to managing large-scale model training."
  -  "Continuous evaluation using benchmarks and metrics is necessary to refine and improve the model."
date: last-modified
bibliography: references.bib
citation:
  container-title: BARG Curtin University
number-sections: false
---

## Introduction

While prompt engineering, Retrieval-Augmented Generation (RAG), and fine-tuning approaches can handle the majority of LLM use cases, there are situations where it may make sense to build a large language model (LLM) from scratch. This white paper reviews the key aspects of developing a foundational LLM based on the experiences with models such as GPT-3, LLaMA, Falcon, and others.

Historically, training large-scale language models (10B+ parameters) was an esoteric activity reserved for AI researchers. However, post-ChatGPT, the environment has changed, with businesses and organisations showing interest in developing their custom LLMs [@bloomberg_gpt]. Although this is unnecessary for over 99% of LLM applications, understanding the process and knowing when it makes sense to build such models is beneficial.

## Financial Considerations

Before diving into the technical aspects of LLM development, it is crucial to consider the financial costs. Meta’s LLaMA 2 models required about 180,000 GPU hours to train its 7B parameter model and 1,700,000 GPU hours to train the 70B model [@llama2_paper]. Translating this into commercial cloud computing costs, an NVIDIA A100 GPU, used for training LLaMA 2 models, costs around $1–2 per GPU hour. Therefore, a ~10B parameter model costs about $150,000 to train, while a ~100B parameter model costs approximately $1,500,000.

Alternatively, one could buy the GPUs instead of renting them. The cost of training would then include the price of the A100 GPUs and the marginal energy costs for model training. An A100 GPU costs about $10,000, and a cluster of 1000 GPUs would amount to approximately $10,000,000. Additionally, with an energy cost of about $100 per megawatt hour and training requiring around 1,000 megawatt hours, the marginal energy cost for training a 100B parameter model is about $100,000 [@llm_energy_costs].

These costs do not include funding a team of ML engineers, data engineers, data scientists, and others needed for model development, which can easily amount to $1,000,000. Hence, training an LLM from scratch is a massive investment, justifiable only if there is a significant potential upside not achievable via prompt engineering or fine-tuning existing models.

## Key Steps in Developing an LLM

The development process of an LLM consists of four key steps:

1. **Data Curation**
2. **Model Architecture**
3. **Training at Scale**
4. **Evaluation**

### Step 1: Data Curation

The quality of an LLM is driven by the quality of its training data. Popular base models have training sets of substantial size:

- GPT-3 (175B): 0.5T tokens [@brown2020language]
- LLaMA (70B): 2T tokens [@llama2_paper]
- Falcon (180B): 3.5T tokens [@falcon180b_blog]

A trillion words of text equate to about a million novels or a billion news articles. The internet is the most common data source for LLMs, encompassing web pages, books, scientific articles, code bases, and conversational data. Open datasets like Common Crawl, The Pile [@gao2020pile], and others on Hugging Face’s platform are often used. Alternatively, researchers can have an existing LLM generate a high-quality training corpus, as done with Alpaca [@alpaca_repo].

Preparing the data involves several preprocessing steps:

- **Quality Filtering:** Removing low-quality text using classifier-based or heuristic-based approaches [@zhao2023survey].
- **De-duplication:** Ensuring the same text does not bias the model or disrupt the training process [@lee2021dedup].
- **Privacy Redaction:** Removing personally identifiable information to avoid unintended exposure of sensitive data.
- **Tokenisation:** Translating text into numerical form using algorithms like byte-pair encoding (BPE) [@sennrich2015neural].

### Step 2: Model Architecture

Transformers are the state-of-the-art approach for language modeling [@vaswani2017attention]. They consist of two key modules: an encoder and a decoder. These can be standalone or combined, resulting in three types of transformers:

1. **Encoder-only:** Suitable for tasks requiring input understanding, such as text classification. Example: BERT [@devlin2018bert].
2. **Decoder-only:** Used for text generation tasks, like GPT-3, LLaMA, and Falcon [@zhao2023survey, huggingface_nlp_course].
3. **Encoder-Decoder:** Combines both modules for tasks like translation and summarisation. Example: BART [@lewis2019bart].

Other design choices include:

- **Residual Connections:** Improve training stability and performance by allowing intermediate values to bypass hidden layers [@he2016deep].
- **Layer Normalisation:** Speeds up training and makes it more stable [@ba2016layer].
- **Activation Functions:** Introduce non-linearities to capture complex mappings (e.g., GeLU, ReLU) [@zhao2023survey].
- **Position Embeddings:** Capture token positions in the model’s representation of text [@vaswani2017attention, shaw2018self].

### Step 3: Training at Scale

LLMs are trained via self-supervised learning, predicting the final token in a sequence based on the preceding ones. Scaling up model training involves techniques like mixed precision training, 3D parallelism, and the sero Redundancy Optimiser (seRO).

**Training Techniques:**

- **Mixed Precision Training:** Uses both 32-bit and 16-bit floating-point data types to reduce computational costs [@zhao2023survey, nvidia_mixed_precision].
- **Parallelisation:** Distributes training across multiple GPUs using pipeline, model, and data parallelism [@zhao2023survey].
- **sero Redundancy Optimiser (seRO):** Reduces data redundancy in the optimiser state, gradient, or parameter partitioning [@zhao2023survey].

**Training Stability:**

- **Checkpointing:** Saves model artifacts to resume training after failures [@zhao2023survey].
- **Weight Decay:** Penalises large parameter values to avoid overfitting [@weight_decay].
- **Gradient Clipping:** Rescales gradients to prevent the exploding gradient problem [@gradient_clipping].

**Hyperparameters:**

- **Batch Sise:** Number of samples processed before updating parameters.
- **Learning Rate:** Controls optimisation step size.
- **Optimiser:** Defines how to update model parameters.
- **Dropout:** zeros out a portion of parameters at random to avoid over fitting [@karpathy_lecture].

### Step 4: Evaluation

Model development is iterative, involving repeated evaluation of model performance on a set of tasks. Common benchmarks include:

- **ARC:** Grade-school level multiple-choice science questions [@clark2018think].
- **Hellaswag:** Commonsense natural language inference tasks [@zellers2019hellaswag].
- **MMLU:** World knowledge and problem-solving tasks [@hendrycks2020measuring].
- **TruthfulQA:** Evaluates model truthfulness against common misconceptions [@lin2021truthfulqa].

Evaluations can be done using prompt templates, human evaluation, or NLP metrics like Perplexity, BLEU, or ROGUE scores. An auxiliary fine-tuned LLM can also be used for comparisons, as demonstrated by GPT-judge [@lin2021truthfulqa].

## Conclusion

Building an LLM from scratch involves significant technical and financial investments. However, understanding this process can help determine when such an endeavour is justified and what the key considerations are. Whether leveraging existing models or building new ones, the goal is to develop an AI solution that effectively addresses the target application.

### References

::: {#refs}
:::