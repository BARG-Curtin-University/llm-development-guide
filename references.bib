@article{bloomberg_gpt,
  title={BloombergGPT},
  note={Paper}
}

@article{llama2_paper,
  title={Llama 2},
  note={Paper}
}

@article{llm_energy_costs,
  title={LLM Energy Costs},
  note={}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{falcon180b_blog,
  title={Falcon 180b},
  note={Blog}
}

@article{gao2020pile,
  title={The Pile: An 800GB dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sidney and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Ameet and Nabeshima, Rishi and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{alpaca_repo,
  title={Alpaca},
  note={Repo}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhang, Yuan and Zou, Yichang and Chen, Jian-Yun Nie},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{lee2021dedup,
  title={Deduplicating training data makes language models better},
  author={Lee, Kenton and Roit, Patrick and Razeghi, Yasaman and Xu, Yi and Min, Sewon and Mukherjee, Semih Yavuz and Raffel, Colin and Petroni, Fabio and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@misc{sentencepiece_repo,
  title={SentencePiece},
  note={Repo}
}

@misc{tokenizers_doc,
  title={Tokenizers},
  note={Doc}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@misc{karpathy_lecture,
  title={Andrej Karpathy Lecture},
  note={}
}

@misc{huggingface_nlp_course,
  title={Hugging Face NLP Course},
  note={}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lewis2019bart,
  title={BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={arXiv preprint arXiv:1603.05027},
  year={2016}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Millican, Katie and Lespiau, Jean-Baptiste and Lai, Alana and Aslanides, John and Henderson, Stephen and Ring, Roman and Keysers, Daniel and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@misc{nvidia_mixed_precision,
  title={Trained with Mixed Precision},
  note={Nvidia Doc}
}

@misc{deepspeed_doc,
  title={DeepSpeed},
  note={Doc}
}

@misc{weight_decay,
  title={Weight Decay},
  note={https://paperswithcode.com/method/weight-decay}
}

@misc{gradient_clipping,
  title={What is Gradient Clipping},
  note={https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{clark2018think,
  title={Think you have solved question answering? Try ARC, the AI2 reasoning challenge},
  author={Clark, Peter and Cowhey, Stephen and Etzioni, Oren and Khot, Tushar and Mishra, Bhavana and Richardson, Kyle and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zellers2019hellaswag,
  title={HellaSwag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{lin2021truthfulqa,
  title={TruthfulQA: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@misc{huggingface_mmlu_leaderboard,
  title={Evaluating MMLU Leaderboard},
  note={https://huggingface.co/blog/evaluating-mmlu-leaderboard}
}
